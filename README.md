# ğŸ¤Ÿ Assistive Technology for Deaf and Mute Communication

This project leverages computer vision and deep learning to **translate sign language gestures into text**, enabling real-time communication for the deaf and mute community.

---

## âœ¨ Features

- ğŸ“· **Real-time gesture detection** using webcam
- ğŸ§  **CNN + LSTM** model to classify sign language gestures
- âš™ï¸ **FastAPI backend** for live stream inference
- ğŸ³ Containerized with Docker for easy deployment
- â˜ï¸ Designed for low-latency cloud deployment (AWS ECS ready)

---

## ğŸš€ Tech Stack

| Component        | Tools Used                          |
|------------------|-------------------------------------|
| Frontend         | OpenCV, Webcam Stream               |
| Model            | PyTorch (CNN + LSTM)                |
| Backend          | FastAPI, Python                     |
| Deployment       | Docker, AWS ECS                     |

---

## ğŸ“ Project Structure

